---
title: "Assignment 2"
author: "Juan D. Gelvez"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
library(glmnet)
library(caret)
```

## Data

For this exercise we use the Communities and Crime data from the UCI ML repository, which includes information about communities in the US. "The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR"

Source: https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime

First, some data prep.

```{r}
crime <- read.csv("communities.data", header = FALSE, na.strings = "?")
varnames <- read.delim("communities.txt", header = FALSE)
```

Clean name vector and use as variable names.

```{r}
varnames <- as.character(varnames$V1)
varnames <- gsub("@attribute ", "", varnames)
varnames <- gsub(" numeric", "", varnames)
varnames <- gsub(" string", "", varnames)
names(crime) <- varnames
```

To make things easier, drop columns with missing values.

```{r}
crime <- crime[, colSums(is.na(crime)) == 0]
```

Check whats left.

```{r}
str(crime)
```

## Train and test set

Next, we want to split the data into a training (75%) and a test (25%) part. This can be done by random sampling with `sample`. Note that there is a `fold` variable in the data set, but here we want to follow our own train/test procedure.

```{r}
set.seed(3940)

train <- sample(1:nrow(crime), size = floor(0.75 * nrow(crime)))
# Subset the data into training and test sets using the sample index
crime_train <- crime[train, ]
crime_test <- crime[-train, ]


```

Now, prepare the training data for running regularized regression models via `glmnet`. Our prediction outcome is `ViolentCrimesPerPop`. As X, use all variables except `state`, `communityname`, and `fold`. 

```{r}

x <- as.matrix(crime_train[, !(names(crime_train) %in% c("state", "communityname", "fold"))])

y <- crime_train$ViolentCrimesPerPop



```

Check whether X looks ok.

```{r}
dim(x)
```

### Lasso

Estimate a sequence of Lasso models using `glmnet`. You can stick with the defaults for choosing a range of lambdas.

```{r}
lasso_1 <- glmnet(x, y, alpha = 1)
lasso_1

```

Here we want to display lambda and the coefficients of the first Lasso model.

```{r}
lasso_1$lambda[1]
```

Same for the last Lasso model.

```{r}
lasso_1$lambda[ncol(lasso_1$beta)]

```

Now, plot the coefficient paths.

```{r}
plot(lasso_1, label=T, xvar = "lambda")
```

Next, we need to decide which Lasso model to pick for prediction. Use Cross-Validation for this purpose.

```{r}
lasso_1_cv <- cv.glmnet(x, y, alpha = 1)

```

And plot the Cross-validation results.

```{r}
plot(lasso_1_cv)

```

In your own words, briefly describe the CV plot. (1) What is plotted here, (2) what can you infer about the relation between the number of variables and prediction accuracy? 

The x-axis represents the logarithm of the lambda values tested during cross-validation; whereas the y-axis represents the mean squared error (MSE) for each model corresponding to each lambda value.The red dots indicate the average MSE for each lambda, and the vertical bars represent the standard error of the mean for these errors. The dotted vertical line on the left indicates the value of lambda that minimizes the mean cross-validated error (lambda.min).

The graph shows that as lambda increases (moving from right to left on the x-axis), the number of variables included in the model typically decreases because larger values of lambda impose a stricter penalty on the model coefficients, driving more of them to zero. When lambda is too small (towards the right side of the plot), the model may become overfit, including too many variables and not generalizing well, which can lead to higher MSE on new, unseen data.



#### end

Now, store the lambda value of the model with the smallest CV error as `bestlam1`.

```{r}
bestlam1 <-lasso_1_cv$lambda.min

```

Create `bestlam2` as the lambda according to the 1-standard error rule.

```{r}
bestlam2 <- lasso_1_cv$lambda.1se

```

### Prediction in test set

Finally, we investigate the performance of our models in the test set. For this task, construct a X matrix from the test set.

```{r}
Xt <- as.matrix(crime_test[, !(names(crime_test) %in% c("state", "communityname", "fold"))])

```

Use the `predict` function to generate predicted values for both models (i.e., both lambdas stored earlier).

```{r}
# Predictions with the best lambda according to the smallest CV error
p_bestlam1 <- predict(lasso_1_cv, newx = Xt, s = bestlam1, type = "response")

# Predictions with the best lambda according to the 1-standard error rule
p_bestlam2 <- predict(lasso_1_cv, newx = Xt, s = bestlam2, type = "response")

```

Compute the test MSE of our models.

```{r}
y_test <- crime_test$ViolentCrimesPerPop

# Compute the test MSE for the model with the best lambda according to the smallest CV error
mse_bestlam1 <- mean((y_test - p_bestlam1)^2)

# Compute the test MSE for the model with the best lambda according to the 1-standard error rule
mse_bestlam2 <- mean((y_test - p_bestlam2)^2)

# Print the test MSE for both models
mse_bestlam1
mse_bestlam2


```

In addition, use another performance metric and compute the corresponding values for both models.

```{r}
# Calculate Mean Absolute Error (MAE) for both models
mae_bestlam1 <- mean(abs(p_bestlam1 - y_test))
mae_bestlam2 <- mean(abs(p_bestlam2 - y_test))

# Output the MSE and MAE for both models
list(
  mse_bestlam1 = mse_bestlam1,
  mse_bestlam2 = mse_bestlam2,
  mae_bestlam1 = mae_bestlam1,
  mae_bestlam2 = mae_bestlam2
)
```

Which model is better? Does it depend on the performance measure that is used?

It seems that both models have identical performance metrics on the test set, with both the Mean Squared Error (MSE) and the Mean Absolute Error (MAE) being exactly the same for bestlam1 and bestlam2. This can happen when the difference in the complexity of the models chosen by bestlam1 and bestlam2 does not significantly affect the predictions on the test set.

#### end
