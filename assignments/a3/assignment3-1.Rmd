---
title: 'Assignment 4: Ensemble Methods'
author: "Juan D. Gelvez"
output:
  html_document:
    df_print: paged
---

## Setup

```{r results='hide', message=FALSE, warning=FALSE}
rm(list=ls())
library(mlbench)
library(foreach)
library(caret)
library(rpart)
#library(learnr)
library(randomForest)
library(e1071)
library(class)
```

## Data

In this notebook, we use the Boston Housing data set (again). "This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston Mass. It was obtained from the StatLib archive (http://lib.stat.cmu.edu/datasets/boston), and has been used extensively throughout the literature to benchmark algorithms."

Source: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html

```{r}
data(BostonHousing2)
names(BostonHousing2)
```

First, we drop some variables that we will not use in the next sections.

```{r}
BostonHousing2$town <- NULL
BostonHousing2$tract <- NULL
BostonHousing2$cmedv <- NULL
```

Next, we start by splitting the data into a train and test set.

```{r}
set.seed(1293)
train <- sample(1:nrow(BostonHousing2), 0.8*nrow(BostonHousing2))
boston_train <- BostonHousing2[train,]
boston_test <- BostonHousing2[-train,]
```

---

#### 1) Bagging with Trees

**a) Build a Bagging model using a `foreach` loop. Use the `maxdepth` control option to grow very small trees. These don't have to be stumps, but should not be much larger than a few splits.**

```{r}
set.seed(123) 
# Define the control with maxdepth to grow small trees
ctrl <- rpart.control(maxdepth = 3) 
# Bagging model using foreach loop
predictions <- foreach(m = 1:100, .combine = cbind) %do% {
  index <- sample(1:nrow(boston_train), replace = TRUE)
  train_sample <- boston_train[index, ]
  fit <- rpart(medv ~ ., data = train_sample, method = "anova", control = ctrl)
  predict(fit, newdata = boston_test)
}

# Calculate the mean of the predictions
mean_predictions <- rowMeans(predictions)

# Evaluate the model
actual <- boston_test$medv
predicted <- mean_predictions

# Using RMSE as the metric for evaluation
postResample(predicted, actual)
```

**b) Plot the last tree of the ensemble to check tree size.**

```{r}
plot(fit, uniform = TRUE)
text(fit, use.n = TRUE)
```


**c) Compare the performance of the last tree in the bagging process with the ensemble. That is, look at the performance of the last tree in the loop and compare it with the performance in the overall averaged bagging model.**

```{r}

last_tree_predictions <- predict(fit, newdata = boston_test) # Predictions from the last tree
last_tree_performance <- postResample(last_tree_predictions, boston_test$medv) # Evaluate the last tree's performance
#ensemb performance
ensemble_performance <- postResample(mean_predictions, boston_test$medv)


cat("Last Tree RMSE:", last_tree_performance[1], "\n")
cat("Ensemble RMSE:", ensemble_performance[1], "\n")
```
The RMSE is lower for the Bagging Ensemble (3.766512) compared to the Last Tree (4.688457). This shows that bagging effectively reduces the variance of the prediction errors. 

---

#### 2) Bagging with Bigger Trees

**a) In the first loop we've grown small trees. Now, build a new loop and adjust `maxdepth` such that very large trees are grown as individual pieces of the Bagging model.**

```{r}
ctrl2 <- rpart.control(maxdepth = 30) # Example of a large depth, or simply omit this parameter

predictions_large_trees <- foreach(m = 1:100, .combine = cbind) %do% {
  index <- sample(1:nrow(boston_train), replace = TRUE)
  train_sample <- boston_train[index, ]
  fit_large <- rpart(medv ~ ., data = train_sample, method = "anova", control = ctrl2)
  predict(fit_large, newdata = boston_test)
}

mean_predictions_large_trees <- rowMeans(predictions_large_trees)
predicted_large_trees <- mean_predictions_large_trees

performance_large_trees <- postResample(predicted_large_trees, actual)
cat("RMSE for Bagging with Large Trees:", performance_large_trees[1], "\n")
#notice that the RMSE is smaller than the previous Bagging Ensemble
```

**b) Confirm that these trees are larger by plotting the last tree.**

It is bigger compared to the previous one: 

```{r}
plot(fit_large, uniform = TRUE)
text(fit_large, use.n = TRUE)
```

**c) Show how this ensemble model performs.**

```{r}
last_tree_predictions_large <- predict(fit_large, newdata = boston_test) # Predictions from the last tree
last_tree_performance_large <- postResample(last_tree_predictions_large, boston_test$medv) # Evaluate the last tree's performance
#ensemb performance
ensemble_performance_large <- postResample(mean_predictions_large_trees, boston_test$medv)


cat("Last Tree RMSE Large:", last_tree_performance_large[1], "\n")
cat("Ensemble RMSE Large:", ensemble_performance_large[1], "\n")
```

The RMSE is lower for the Bagging Ensemble (3.70823) compared to the Last Tree (5.103384 ). This shows that bagging effectively reduces the variance of the prediction errors. 


**d) In summary, which setting of `maxdepth` did you expect to work better? Why?**


The "best" setting for maxdepth depends on the specific characteristics of the dataset and the general goals. However, a smaller maxdepth limits the growth of the tree, leading to simpler models that are less likely to overfit the training data but a small tree might not adequately learn the relationships in the data, resulting in underfitting. In this case, where the dataset might be complex and have low noise, a larger maxdepth might work better because it allows the model to capture the complexity. This can be seen by comparing the RMSE where with large trees where 3.70823 whereas with small trees were 3.766512


#### 3) Building a Boosting Model with XGBoost

**a) Now let's try using a boosting model using trees as the base learner. Here, we will use the XGBoost model. First, set up the `trainControl` parameters.**


```{r}

# Setting 5-fold cross-validation
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = "final",
                     verboseIter = TRUE)
```

**b) Next, set up the tuning parameters by creating a grid of parameters to try.**

```{r}
tuneGrid <- expand.grid(nrounds = c(10, 20),
                        max_depth = c(3, 6, 9),
                        eta = c(0.01, 0.1),
                        gamma = c(0, 0.1),
                        colsample_bytree = c(0.5, 0.75),
                        min_child_weight = c(1, 3),
                        subsample = c(0.5, 0.75))


```

**c) Using CV to tune, fit an XGBoost model.**

```{r}
library(xgboost)
# Fitting the XGBoost model
xgbModel <- train(medv ~ .,
                  data = boston_train,
                  method = "xgbTree",
                  trControl = ctrl,
                  tuneGrid = tuneGrid,
                  metric = "RMSE")

```


**d) Compare the performance of the boosting model with the models run previously in this assignment. How does it compare?**

```{r}

cat("Ensemble RMSE Small:", ensemble_performance[1], "\n") #smalltree
cat("Ensemble RMSE Large:", ensemble_performance_large[1], "\n") #bigtree


pred_xgb <- predict(xgbModel, newdata = boston_test)
results_xgb <- postResample(pred_xgb, boston_test$medv)
results_xgb
```
The results are as follows, suggesting better performance of larger trees than small trees or XGBModels. 

XGB MAE is 4.3971496 
Baggin with small tree 3.766512
Bagging with big tree 3.70823



#### 4) Comparing Models with `caretList`

**a) Use `caretList` to run a Bagging model, a Random Forest model, and an XGBoost model using the same CV splits with 5-fold CV. Plot the performance by RMSE. How do the models compare?**

*Hint: You can use `treebag`, `ranger`, and `xgbTree` for the models.*

```{r}
library(caret)
library(caretEnsemble)
?caretList
ctrl <- trainControl(method = "cv",
                     number = 5,
                     savePredictions = "final",
                     allowParallel = TRUE)

models <- caretList(medv ~ .,
                    data = boston_train,
                    trControl = ctrl,
                    methodList = c("treebag", "ranger", "xgbTree"))

results <- resamples(models)
dotplot(results, metric = "RMSE")
```

Lower average RMSE values indicate better average performance across the cross-validation folds. The lines represent the confidence intervals around the average RMSE (here at a 95% confidence level), indicating the range within which we can be confident that the true mean RMSE lies. 

The xgbTree model appears to be the best performer in terms of both average RMSE and consistency of performance across CV folds.However it seems that is not statistically different from the other two. It is important to also consider other metrics like MAE


